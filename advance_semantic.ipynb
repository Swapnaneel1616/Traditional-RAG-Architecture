{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db5d2802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bb44ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 948.11it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14b7fbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample text\n",
    "text=\"\"\"\n",
    "LangChain is a framework for building applications with LLMs.\n",
    "LangChain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.\n",
    "You can create chains, agents, memory, and retrievers.\n",
    "The Eiffel Tower is located in Paris.\n",
    "France is a popular tourist destination.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e35171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [s.strip() for s in text.split(\"\\n\") if s.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "116b20e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LangChain is a framework for building applications with LLMs.',\n",
       " 'LangChain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.',\n",
       " 'You can create chains, agents, memory, and retrievers.',\n",
       " 'The Eiffel Tower is located in Paris.',\n",
       " 'France is a popular tourist destination.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ec2ffc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk1:\n",
      "LangChain is a framework for building applications with LLMs. LangChain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.\n",
      "\n",
      "Chunk2:\n",
      "You can create chains, agents, memory, and retrievers.\n",
      "\n",
      "Chunk3:\n",
      "The Eiffel Tower is located in Paris.\n",
      "\n",
      "Chunk4:\n",
      "France is a popular tourist destination.\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.encode(sentence)\n",
    "threshold = 0.7\n",
    "chunk = []\n",
    "current_chunk = [sentence[0]]\n",
    "\n",
    "for i in range(1,len(sentence)):\n",
    "    sim = cosine_similarity(\n",
    "        [embeddings[i-1]],\n",
    "        [embeddings[i]]\n",
    "    )[0][0]\n",
    "    if sim>=threshold:\n",
    "        current_chunk.append(sentence[i])\n",
    "    else:\n",
    "        chunk.append(\" \".join(current_chunk))\n",
    "        current_chunk = [sentence[i]]\n",
    "    \n",
    "chunk.append(\" \".join(current_chunk))\n",
    "\n",
    "for idx , ck in enumerate(chunk):\n",
    "    print(f\"\\nChunk{idx+1}:\\n{ck}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a713575",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAG PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "693b3594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import TextLoader , PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate , PromptTemplate\n",
    "from langchain_classic.chains import create_history_aware_retriever , create_retrieval_chain\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.messages import HumanMessage , AIMessage \n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough , RunnableMap , RunnableLambda\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec03720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom Semantic Chunker With Threshold\n",
    "class ThresholdSemanticChunker:\n",
    "    def __init__(self, model_name = \"all-MiniLM-L6-v2\",threshold=0.7):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def split(self , text:str):\n",
    "        sentence = [s.strip() for s in text.split(\"\\n\") if s.strip()]\n",
    "        embeddings = self.model.encode(sentence)\n",
    "        chunk = []\n",
    "        current_chunk = [sentence[0]]\n",
    "\n",
    "        for i in range(1,len(sentence)):\n",
    "            sim = cosine_similarity(\n",
    "                [embeddings[i-1]],\n",
    "                [embeddings[i]]\n",
    "            )[0][0]\n",
    "            if sim>=self.threshold:\n",
    "                current_chunk.append(sentence[i])\n",
    "            else:\n",
    "                chunk.append(\" \".join(current_chunk))\n",
    "                current_chunk = [sentence[i]]\n",
    "            \n",
    "        chunk.append(\" \".join(current_chunk))\n",
    "        return chunk\n",
    "    \n",
    "    def split_documents(self,docs):\n",
    "        result = []\n",
    "        for doc in docs:\n",
    "            for chunk in self.split(doc.page_content):\n",
    "                result.append(Document(page_content=chunk,metadata=doc.metadata))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a731ce8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content='\\nLangChain is a framework for building applications with LLMs.\\nLangChain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.\\nYou can create chains, agents, memory, and retrievers.\\nThe Eiffel Tower is located in Paris.\\nFrance is a popular tourist destination.\\n')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = Document(page_content=text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b85fceb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 786.29it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='LangChain is a framework for building applications with LLMs. LangChain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.'),\n",
       " Document(metadata={}, page_content='You can create chains, agents, memory, and retrievers.'),\n",
       " Document(metadata={}, page_content='The Eiffel Tower is located in Paris.'),\n",
       " Document(metadata={}, page_content='France is a popular tourist destination.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###CHUNKING\n",
    "chunker = ThresholdSemanticChunker(threshold=0.7)\n",
    "chunks = chunker.split_documents([doc])\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d42c4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(chunks , OpenAIEmbeddings())\n",
    "retriever = vector_store.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95bc8cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based on the following context:\\n\\n{context}\\n\\n\\nQuestion: {question}\\n\\n')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Answer the question based on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c930156",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(model=\"groq:qwen/qwen3-32b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c03850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    RunnableMap(\n",
    "        {\n",
    "            \"context\": lambda x: retriever.invoke(x[\"question\"]),\n",
    "            \"question\": lambda x: x['question']\n",
    "        }\n",
    "    )|prompt|llm|StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9921e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so the user is asking, \"What is LangChain used for?\" Let me look at the provided documents to find the answer.\n",
      "\n",
      "First, the document with ID '4a1b69bd-c7b1-4403-8f92-77813e83ae54' says, \"LangChain is a framework for building applications with LLMs. LangChain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.\" That seems directly relevant. It mentions that LangChain is used for building applications using LLMs (Large Language Models) and that it helps combine them with other tools.\n",
      "\n",
      "Another document, 'dd84c688-2426-4290-b76e-59b72a968ce2', states, \"You can create chains, agents, memory, and retrievers.\" This adds more specifics about the components or features that LangChain offers. Chains, agents, memory, and retrievers are probably parts of the framework that help in structuring applications with LLMs.\n",
      "\n",
      "The other two documents talk about France and the Eiffel Tower, which don't relate to LangChain. So I can ignore those.\n",
      "\n",
      "Putting it all together, LangChain is a framework designed to help developers build applications that utilize LLMs. It provides modular components (like chains, agents, etc.) and integrates with tools such as OpenAI and Pinecone. The answer should mention the purpose (building apps with LLMs) and the key features (modular abstractions, integration with tools, specific components like chains and agents).\n",
      "</think>\n",
      "\n",
      "LangChain is used for building applications with Large Language Models (LLMs). It provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone, and supports the creation of components such as chains, agents, memory, and retrievers to structure and enhance LLM-based applications.\n"
     ]
    }
   ],
   "source": [
    "query = {\"question\": \"What is langchain used for?\"}\n",
    "result = rag_chain.invoke(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0526aba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_Updated",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
