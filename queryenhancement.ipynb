{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b50463ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\Ai_Projects\\langchain_Updated\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import TextLoader , PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate , PromptTemplate\n",
    "from langchain_classic.chains import create_history_aware_retriever , create_retrieval_chain\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.messages import HumanMessage , AIMessage \n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough , RunnableMap\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c769f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and split the dataset\n",
    "loader = TextLoader(\"langchain_crewai_dataset.txt\")\n",
    "raw_docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 300 , chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(raw_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29a252d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embed and store in DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d221c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "vector_store = FAISS.from_documents(chunks , embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d12e6465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retriever MMR\n",
    "retriever = vector_store.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "112db5c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000281E18B44D0>, search_type='mmr', search_kwargs={'k': 5})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eef2c9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='0be58b67-f732-4c44-a21a-8b9894d414b4', metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='LangChain is compatible with multiple LLM providers including OpenAI, Anthropic, Cohere, Hugging Face, and more. This flexibility ensures that developers can switch between models without rewriting core logic. (v2)'),\n",
       " Document(id='a2fb2fad-1175-4e3d-b51f-897717c31e59', metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='LangChain is an open-source framework designed for developing applications powered by large language models (LLMs). It simplifies the process of building, managing, and scaling complex chains of thought by abstracting prompt management, retrieval, memory, and agent orchestration. Developers can use'),\n",
       " Document(id='9321227d-7e5b-484e-a2fe-40d08f419149', metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='LangChain is compatible with multiple LLM providers including OpenAI, Anthropic, Cohere, Hugging Face, and more. This flexibility ensures that developers can switch between models without rewriting core logic. (v9)'),\n",
       " Document(id='b166dc81-1f97-4b03-86b4-288a18dccba2', metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='LangChain is compatible with multiple LLM providers including OpenAI, Anthropic, Cohere, Hugging Face, and more. This flexibility ensures that developers can switch between models without rewriting core logic. (v10)'),\n",
       " Document(id='71aad655-0677-4822-bbbf-48eea9507ca3', metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='LangChain is compatible with multiple LLM providers including OpenAI, Anthropic, Cohere, Hugging Face, and more. This flexibility ensures that developers can switch between models without rewriting core logic. (v8)')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"Langchain Models?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e00217e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 16384, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x00000281E628DF50>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000281E628D690>, model_name='qwen/qwen3-32b', reasoning_format='hidden', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM and PROMPT\n",
    "llm = init_chat_model(model=\"groq:qwen/qwen3-32b\" , reasoning_format=\"hidden\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f566420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='You are a helpful assistant.Expand the following query to improve document retrieval \\n    by adding relevant synonyms , technical terms , useful context\\n\\n    Original query: \"{query}\"\\n\\n    Expanded query: \\n\\n    ')\n",
       "| ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 16384, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x00000281E628DF50>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000281E628D690>, model_name='qwen/qwen3-32b', reasoning_format='hidden', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Query Expnasion\n",
    "query_expansion_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful assistant.Expand the following query to improve document retrieval \n",
    "    by adding relevant synonyms , technical terms , useful context\n",
    "    \n",
    "    Original query: \"{query}\"\n",
    "\n",
    "    Expanded query: \n",
    "\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "query_expansion_chain = query_expansion_prompt|llm|StrOutputParser()\n",
    "query_expansion_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5897db40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Expanded Query:**  \\n\"Langchain Memory\" OR \"LangChain Memory\" OR \"Langchain memory framework\" OR \"LangChain memory module\"  \\nAND (  \\n    \"state management\" OR \"context retention\" OR \"session memory\" OR \"conversation history\"  \\n    OR \"memory components\" OR \"memory modules\" OR \"persisted memory systems\"  \\n    OR \"LLM memory integration\" OR \"agent-based memory\" OR \"long-term memory storage\"  \\n)  \\nAND (  \\n    \"Langchain framework\" OR \"LangChain library\" OR \"Langchain RAG systems\" OR \"Langchain AI application architecture\"  \\n    OR \"Langchain chatbot memory\" OR \"Langchain agent memory\" OR \"Langchain memory optimization\"  \\n    OR \"Langchain memory implementation\" OR \"Langchain memory patterns\"  \\n)  \\nAND (  \\n    \"conversation buffer memory\" OR \"token retention\" OR \"context window management\"  \\n    OR \"memory persistence\" OR \"memory caching\" OR \"memory lifecycle\"  \\n    OR \"Langchain Memory class\" OR \"Langchain Memory API\"  \\n)  \\nAND (  \\n    \"AI chatbot development\" OR \"large language model (LLM) memory\" OR \"NLP session tracking\"  \\n    OR \"Langchain vs. other memory frameworks\" OR \"Langchain memory use cases\"  \\n)  \\n\\n**Rationale:**  \\n1. **Synonyms/Variants**: Covers capitalization (LangChain vs. Langchain) and pluralization.  \\n2. **Technical Terms**: Includes specific LangChain components (e.g., \"Memory class\"), memory types (e.g., \"conversation buffer\"), and LLM-related concepts (e.g., \"context window\").  \\n3. **Use Cases**: Expands to applications like chatbots, RAG systems, and agent-based workflows.  \\n4. **Comparative/Contextual Terms**: Adds terms for contrasting LangChain with other frameworks or exploring its implementation depth.  \\n\\nThis ensures broader coverage of documents discussing LangChain’s memory architecture, implementation strategies, and application scenarios.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_expansion_chain.invoke({\"query\":\"Langchain Memory\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5779531",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAG ANSWERING PROMPT\n",
    "answer_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based on the context below\n",
    "    \n",
    "    Context: {context}\n",
    "\n",
    "    Question: {input}\n",
    "    \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm=llm , prompt=answer_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2b41228",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full RAG Pipeline\n",
    "rag_pipeline = (\n",
    "    RunnableMap({\n",
    "        \"input\": lambda x : x[\"input\"],\n",
    "        \"context\": lambda x : retriever.invoke(query_expansion_chain.invoke({\"query\":x[\"input\"]}))\n",
    "    })| document_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e36b22fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Expanded Query:**  \n",
      "{\"input\": \"What types of memory, storage, or persistence mechanisms does Langchain support? Please include details on memory modules, session handling, caching strategies, state management, or integration with databases (e.g., in-memory databases, key-value stores, SQL/NoSQL). Clarify if Langchain supports transient vs. persistent memory, chatbot session memory, agent memory systems, or specialized memory configurations for tasks like conversational agents, task automation, or multi-turn interactions.\"}  \n",
      "\n",
      "**Rationale for Expansion:**  \n",
      "1. **Synonyms & Broader Terms:** Added \"storage,\" \"persistence,\" and \"mechanisms\" to capture alternative phrasing.  \n",
      "2. **Technical Terms:** Included \"memory modules,\" \"caching strategies,\" \"state management,\" and specific database types (in-memory, key-value, SQL/NoSQL) to target technical documentation.  \n",
      "3. **Use Cases:** Highlighted \"chatbot session memory,\" \"agent memory systems,\" and \"multi-turn interactions\" to align with Langchain’s typical applications.  \n",
      "4. **Clarification Requests:** Distinguished between \"transient vs. persistent memory\" and \"specialized memory configurations\" to address potential implementation nuances.  \n",
      "5. **Integration Context:** Added \"integration with databases\" to explore external storage options.  \n",
      "\n",
      "This expansion ensures comprehensive retrieval of documentation, tutorials, or API references related to Langchain’s memory architecture.\n",
      "Answer\n",
      " LangChain supports two types of memory modules:  \n",
      "1. **ConversationBufferMemory**: Maintains a buffer of previous conversation turns to keep the LLM aware of the interaction history.  \n",
      "2. **ConversationSummaryMemory**: Summarizes long interactions to fit within token limits while preserving key context.  \n",
      "\n",
      "These modules enable the LLM to retain awareness of prior dialogue. The mention of hybrid retrieval (combining sparse and dense methods) in the context refers to retrieval strategies, not memory types.\n"
     ]
    }
   ],
   "source": [
    "#Query \n",
    "query = {\"input\": \"What types of memory does Langchain support?\"}\n",
    "print(query_expansion_chain.invoke({\"query\":query}))\n",
    "response = rag_pipeline.invoke(query)\n",
    "print(\"Answer\\n\",response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32e52d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_Updated",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
