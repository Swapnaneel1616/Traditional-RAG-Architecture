{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26a2f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "import numpy as np\n",
    "from typing import List\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ['GROQ_API_KEY'] = os.getenv(\"GROQ_API_KEY\")\n",
    "model = init_chat_model(\"groq:qwen/qwen3-32b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8cfd994",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_docs = [\n",
    "    \"\"\"\n",
    "    Retrieval-Augmented Generation (RAG) and Vector Databases\n",
    "    \n",
    "    RAG systems enhance Large Language Models by retrieving relevant data from external sources like ChromaDB or FAISS. \n",
    "    By converting documents into vector embeddings, the system can perform semantic searches to find context \n",
    "    that a model wasn't originally trained on, reducing hallucinations.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    Agentic AI and Autonomous Workflows\n",
    "    \n",
    "    Agentic AI refers to systems designed to use tools and make decisions to achieve a goal. \n",
    "    Unlike standard chatbots, AI agents can use 'Reasoning and Acting' (ReAct) patterns to \n",
    "    call APIs, search the web, or execute code independently to complete multi-step tasks.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    Cloud-Native Microservices and Scalability\n",
    "    \n",
    "    Modern backend architectures often utilize Spring Boot and Docker to create microservices. \n",
    "    Deploying these on AWS using services like EKS or Lambda allows for elastic scaling. \n",
    "    API Gateways act as the entry point, routing traffic to specific services like Product, Order, or User modules.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    Natural Language Processing with NLTK and Transformers\n",
    "    \n",
    "    Natural Language Processing (NLP) involves the interaction between computers and human languages. \n",
    "    Libraries like NLTK are used for basic tokenization and stop-word removal, while Transformer-based \n",
    "    models like BERT or GPT handle complex tasks like sentiment analysis and language translation.\n",
    "    \"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "449315af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\n    Retrieval-Augmented Generation (RAG) and Vector Databases\\n\\n    RAG systems enhance Large Language Models by retrieving relevant data from external sources like ChromaDB or FAISS. \\n    By converting documents into vector embeddings, the system can perform semantic searches to find context \\n    that a model wasn't originally trained on, reducing hallucinations.\\n    \",\n",
       " \"\\n    Agentic AI and Autonomous Workflows\\n\\n    Agentic AI refers to systems designed to use tools and make decisions to achieve a goal. \\n    Unlike standard chatbots, AI agents can use 'Reasoning and Acting' (ReAct) patterns to \\n    call APIs, search the web, or execute code independently to complete multi-step tasks.\\n    \",\n",
       " '\\n    Cloud-Native Microservices and Scalability\\n\\n    Modern backend architectures often utilize Spring Boot and Docker to create microservices. \\n    Deploying these on AWS using services like EKS or Lambda allows for elastic scaling. \\n    API Gateways act as the entry point, routing traffic to specific services like Product, Order, or User modules.\\n    ',\n",
       " '\\n    Natural Language Processing with NLTK and Transformers\\n\\n    Natural Language Processing (NLP) involves the interaction between computers and human languages. \\n    Libraries like NLTK are used for basic tokenization and stop-word removal, while Transformer-based \\n    models like BERT or GPT handle complex tasks like sentiment analysis and language translation.\\n    ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a43ff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save sample\n",
    "import tempfile\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "\n",
    "for i , doc in enumerate(sample_docs):\n",
    "    with open(f\"doc.{i}.txt\", \"w\") as f:\n",
    "        f.write(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00fc575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Document Loading\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    \"Data\",\n",
    "    glob =\"*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={'encoding':'utf-8'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "443e96fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.directory.DirectoryLoader at 0x211c5146d10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45ad5b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c7c3b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Data\\\\doc.0.txt'}, page_content=\"\\n    Retrieval-Augmented Generation (RAG) and Vector Databases\\n\\n    RAG systems enhance Large Language Models by retrieving relevant data from external sources like ChromaDB or FAISS. \\n    By converting documents into vector embeddings, the system can perform semantic searches to find context \\n    that a model wasn't originally trained on, reducing hallucinations.\\n    \"),\n",
       " Document(metadata={'source': 'Data\\\\doc.1.txt'}, page_content=\"\\n    Agentic AI and Autonomous Workflows\\n\\n    Agentic AI refers to systems designed to use tools and make decisions to achieve a goal. \\n    Unlike standard chatbots, AI agents can use 'Reasoning and Acting' (ReAct) patterns to \\n    call APIs, search the web, or execute code independently to complete multi-step tasks.\\n    \"),\n",
       " Document(metadata={'source': 'Data\\\\doc.2.txt'}, page_content='\\n    Cloud-Native Microservices and Scalability\\n\\n    Modern backend architectures often utilize Spring Boot and Docker to create microservices. \\n    Deploying these on AWS using services like EKS or Lambda allows for elastic scaling. \\n    API Gateways act as the entry point, routing traffic to specific services like Product, Order, or User modules.\\n    '),\n",
       " Document(metadata={'source': 'Data\\\\doc.3.txt'}, page_content='\\n    Natural Language Processing with NLTK and Transformers\\n\\n    Natural Language Processing (NLP) involves the interaction between computers and human languages. \\n    Libraries like NLTK are used for basic tokenization and stop-word removal, while Transformer-based \\n    models like BERT or GPT handle complex tasks like sentiment analysis and language translation.\\n    ')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42f1c679",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Splliting \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0 , length_function = len , separators=[\" \"] )\n",
    "\n",
    "chunk = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d36359e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Data\\\\doc.0.txt'}, page_content='Retrieval-Augmented Generation (RAG) and Vector Databases\\n\\n    RAG systems enhance Large'),\n",
       " Document(metadata={'source': 'Data\\\\doc.0.txt'}, page_content='Language Models by retrieving relevant data from external sources like ChromaDB or FAISS. \\n    By'),\n",
       " Document(metadata={'source': 'Data\\\\doc.0.txt'}, page_content='converting documents into vector embeddings, the system can perform semantic searches to find'),\n",
       " Document(metadata={'source': 'Data\\\\doc.0.txt'}, page_content=\"context \\n    that a model wasn't originally trained on, reducing hallucinations.\"),\n",
       " Document(metadata={'source': 'Data\\\\doc.1.txt'}, page_content='Agentic AI and Autonomous Workflows\\n\\n    Agentic AI refers to systems designed to use tools and'),\n",
       " Document(metadata={'source': 'Data\\\\doc.1.txt'}, page_content=\"make decisions to achieve a goal. \\n    Unlike standard chatbots, AI agents can use 'Reasoning and\"),\n",
       " Document(metadata={'source': 'Data\\\\doc.1.txt'}, page_content=\"Acting' (ReAct) patterns to \\n    call APIs, search the web, or execute code independently to\"),\n",
       " Document(metadata={'source': 'Data\\\\doc.1.txt'}, page_content='complete multi-step tasks.'),\n",
       " Document(metadata={'source': 'Data\\\\doc.2.txt'}, page_content='Cloud-Native Microservices and Scalability\\n\\n    Modern backend architectures often utilize'),\n",
       " Document(metadata={'source': 'Data\\\\doc.2.txt'}, page_content='Spring Boot and Docker to create microservices. \\n    Deploying these on AWS using services like EKS'),\n",
       " Document(metadata={'source': 'Data\\\\doc.2.txt'}, page_content='or Lambda allows for elastic scaling. \\n    API Gateways act as the entry point, routing traffic to'),\n",
       " Document(metadata={'source': 'Data\\\\doc.2.txt'}, page_content='specific services like Product, Order, or User modules.'),\n",
       " Document(metadata={'source': 'Data\\\\doc.3.txt'}, page_content='Natural Language Processing with NLTK and Transformers\\n\\n    Natural Language Processing (NLP)'),\n",
       " Document(metadata={'source': 'Data\\\\doc.3.txt'}, page_content='involves the interaction between computers and human languages. \\n    Libraries like NLTK are used'),\n",
       " Document(metadata={'source': 'Data\\\\doc.3.txt'}, page_content='for basic tokenization and stop-word removal, while Transformer-based \\n    models like BERT or GPT'),\n",
       " Document(metadata={'source': 'Data\\\\doc.3.txt'}, page_content='handle complex tasks like sentiment analysis and language translation.')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3fdb2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedding Models \n",
    "embeddings = OpenAIEmbeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e529003",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chroma DB Vector Store \n",
    "persistence_directory = \"./chrom_db\"\n",
    "\n",
    "vectorStore = Chroma.from_documents(\n",
    "    documents=chunk,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    persist_directory=persistence_directory,\n",
    "    collection_name=\"rag_collection\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a8291e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='fed22f43-38e9-4be4-8e35-d2ec3e2d5f9d', metadata={'source': 'Data\\\\doc.1.txt'}, page_content='Agentic AI and Autonomous Workflows\\n\\n    Agentic AI refers to systems designed to use tools and'),\n",
       " Document(id='b7ed2ebe-41fc-444e-88e4-c2fc37b0dcab', metadata={'source': 'Data\\\\doc.3.txt'}, page_content='Natural Language Processing with NLTK and Transformers\\n\\n    Natural Language Processing (NLP)'),\n",
       " Document(id='0a3feb73-abe5-4cf1-bbb1-bb50110b138f', metadata={'source': 'Data\\\\doc.1.txt'}, page_content=\"make decisions to achieve a goal. \\n    Unlike standard chatbots, AI agents can use 'Reasoning and\")]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Text Similarity Search \n",
    "query = \"What refers to Agentic AI?\"\n",
    "\n",
    "similar_docs = vectorStore.similarity_search(query , k=3)\n",
    "similar_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7455193a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(id='fed22f43-38e9-4be4-8e35-d2ec3e2d5f9d', metadata={'source': 'Data\\\\doc.1.txt'}, page_content='Agentic AI and Autonomous Workflows\\n\\n    Agentic AI refers to systems designed to use tools and'),\n",
       "  0.2739553451538086),\n",
       " (Document(id='b7ed2ebe-41fc-444e-88e4-c2fc37b0dcab', metadata={'source': 'Data\\\\doc.3.txt'}, page_content='Natural Language Processing with NLTK and Transformers\\n\\n    Natural Language Processing (NLP)'),\n",
       "  0.4770853817462921),\n",
       " (Document(id='0a3feb73-abe5-4cf1-bbb1-bb50110b138f', metadata={'source': 'Data\\\\doc.1.txt'}, page_content=\"make decisions to achieve a goal. \\n    Unlike standard chatbots, AI agents can use 'Reasoning and\"),\n",
       "  0.4980132281780243)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Advacne similarity search \n",
    "score = vectorStore.similarity_search_with_score(query , k =3)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81a77ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "376ad732",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model= \"qwen/qwen3-32b\",\n",
    "    temperature= 0.2,\n",
    "    max_tokens=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "de4ed546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"<think>\\nOkay, the user is asking what a large language model is. First, I need to define it clearly. A large language model is a type of artificial intelligence that processes and generates human-like text. It's based on deep learning techniques, especially neural networks.\\n\\nNext, I should explain the key components. They are trained on vast amounts of text data, which allows them to understand grammar, context, and even some reasoning. The training process involves predicting the next word in a sentence, which helps them learn patterns and relationships between words.\\n\\nThen, I should mention the different architectures, like the Transformer model, which is widely used. Transformers use attention mechanisms to handle long-range dependencies in text, making them more efficient and effective than older models like RNNs.\\n\\nApplications are important to highlight. They can be used for tasks like translation, summarization, question-answering, and even creative writing. Examples like GPT, BERT, and others can be mentioned to give concrete instances.\\n\\nI should also touch on the training process, including the need for massive datasets and computational resources. This explains why only a few companies can develop such models.\\n\\nPotential challenges and limitations are worth noting too. Issues like data bias, ethical concerns, and the need for continuous updates should be addressed. Also, the difference between understanding and actual knowledge is importantâ€”models don't truly understand but simulate based on patterns.\\n\\nFinally, wrap it up by emphasizing their impact on various fields and the ongoing research to improve them. Make sure the explanation is clear and accessible, avoiding too much jargon but still covering the essentials.\\n</think>\\n\\nA **Large Language Model (LLM)** is an advanced type of artificial intelligence (AI) designed to understand, generate, and manipulate human language. These models are trained on vast amounts of text data from the internet, books, articles, and other sources to learn patterns, grammar, context, and relationships between words and phrases. They can perform tasks like answering questions, writing stories, translating languages, summarizing text, and even coding.\\n\\n### Key Characteristics of LLMs:\\n1. **Scale**:  \\n   - LLMs have billions (or even trillions) of parameters, which are the internal variables that the model adjusts during training to improve its predictions. The larger the model, the more complex patterns it can learn.\\n\\n2. **Training Data**:  \\n   - They are trained on massive datasets, often containing diverse text from books, websites, and other sources.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 500, 'prompt_tokens': 13, 'total_tokens': 513, 'completion_time': 1.1007357660000001, 'completion_tokens_details': None, 'prompt_time': 0.000363375, 'prompt_tokens_details': None, 'queue_time': 0.167779614, 'total_time': 1.101099141}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_d58dbe76cd', 'service_tier': 'on_demand', 'finish_reason': 'length', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c4833-fea7-7583-b1ae-a22db7324dda-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 13, 'output_tokens': 500, 'total_tokens': 513})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is Large Language Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4ab9fd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(\"groq:qwen/qwen3-32b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "86f392f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rag Chain\n",
    "from langchain_classic.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatMessagePromptTemplate\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cd93a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_Updated",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
